###Removing a monitor

when recreating a cluster with a box that used to be in a different cluster remove these packages

service ceph -a stop mon.{mon-id}
Remove the monitor from the cluster.
ceph mon remove {mon-id}
Remove the monitor entry from ceph.conf.

ceph-deploy purgedata {ceph-node} [{ceph-node}]
ceph-deploy forgetkeys
ceph-deploy purge {ceph-node} [{ceph-node}]

yum -y remove python-rados
yum -y remove librados2
yum -y remove python-rbd
yum -y remove python-cephfs
yum -y remove libcephfs1
yum -y remove radosgw

####Removing OSDs
From admin node
ceph osd out {osd-num}
ceph -w #observe effects
From OSD host
ssh {osd-host}
sudo /etc/init.d/ceph stop osd.{osd-num}
ceph osd crush remove {name}
ceph auth del osd.{osd-num}
ceph osd rm {osd-num}
From Admin node remove all entries in ceph.conf

###Creating CEPH Cluster
http://ceph.com/docs/master/rados/deployment/preflight-checklist/

create a cluster directory in / to hold all the conf files

create ceph user
give user passwordless sudo
Configure your admin machine with password-less SSH access to each node running Ceph daemons

install CEPH deploy
vi /etc/yum.repos.d/ceph.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://ceph.com/rpm-{ceph-release}/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc

 yum install ceph-deploy

create new cluster with new monitors
ceph-deploy new ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com

ceph-deploy install ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com

Add the initial monitor(s) and gather the keys:
ceph-deploy mon create-initial

[root@ceph3 ceph]# ls
ceph.bootstrap-mds.keyring  ceph.bootstrap-rgw.keyring  ceph.conf  ceph.mon.keyring
ceph.bootstrap-osd.keyring  ceph.client.admin.keyring   ceph.log

#Maybe not????
#gather keys
#ceph-deploy gatherkeys ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com

#List the drive on the host
ceph-deploy disk list ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com


Blank OSDs with zap
ceph-deploy disk zap ceph3:sda
ceph-deploy disk zap ceph3:sdb
ceph-deploy disk zap ceph3:sdc
ceph-deploy disk zap ceph3:sdd
ceph-deploy disk zap ceph3:sdg
ceph-deploy disk zap ceph3:sdh
ceph-deploy disk zap ceph3:sdi
ceph-deploy disk zap ceph3:sdj
ceph-deploy disk zap ceph3:sdk
ceph-deploy disk zap ceph3:sdl
ceph-deploy disk zap ceph3:sdm
ceph-deploy disk zap ceph3:sdn


Prepare OSDs
ceph-deploy osd prepare {node-name}:{data-disk}[:{journal-disk}]
ceph-deploy osd prepare ceph3:sda:sdg
ceph-deploy osd prepare ceph3:sdb:sdg
ceph-deploy osd prepare ceph3:sdc:sdg
ceph-deploy osd prepare ceph3:sdd:sdg
ceph-deploy osd prepare ceph3:sdi:sdg

ceph-deploy osd prepare ceph3:sdj:sdh
ceph-deploy osd prepare ceph3:sdk:sdh
ceph-deploy osd prepare ceph3:sdl:sdh
ceph-deploy osd prepare ceph3:sdm:sdh
ceph-deploy osd prepare ceph3:sdn:sdh

Activate OSDs
ceph-deploy osd activate {node-name}:{data-disk-partition}[:{journal-disk-partition}]
ceph-deploy --username cephuser osd activate ceph3:sda1:sdg1
ceph-deploy --username cephuser osd activate ceph3:sdb1:sdg2
ceph-deploy --username cephuser osd activate ceph3:sdc1:sdg3
ceph-deploy --username cephuser osd activate ceph3:sdd1:sdg4
ceph-deploy --username cephuser osd activate ceph3:sdi1:sdg5

ceph-deploy --username cephuser osd activate ceph3:sdj1:sdh1
ceph-deploy --username cephuser osd activate ceph3:sdk1:sdh2
ceph-deploy --username cephuser osd activate ceph3:sdl1:sdh3
ceph-deploy --username cephuser osd activate ceph3:sdm1:sdh4
ceph-deploy --username cephuser osd activate ceph3:sdn1:sdh5

#Use ceph-deploy to copy the configuration file and admin key to your admin node and your Ceph Nodes so that you can use the ceph CLI without having to specify the monitor address and ceph.client.admin.keyring each time you execute a command.
ceph-deploy admin {admin-node} {ceph-node}

set keyring permissions
chmod +r /etc/ceph/ceph.client.admin.keyring

Check your clusterâ€™s health.
ceph health

###Test CEPH








####Preparing a KVM server to host VMs that mount CEPH volumes

Build KVM with foreman and apply the foremankvm module
reboot

in DevNet from ceph2 the admin-node
switch to the ceph directory 
on ceph2:/my-cluster/
ceph-deploy install kvm18.hostconfig.mgmt.us1.dev.md.fireeye.com
ceph-deploy admin kvm18.hostconfig.mgmt.us1.dev.md.fireeye.com

Create a secret for libvirt and ceph to share for auth of ceph shares
cat > secret.xml <<EOF
<secret ephemeral='no' private='no'>
<usage type='ceph'>
<name>client.libvirt secret</name>
</usage>
</secret>
EOF

Then :  
virsh secret-define --file secret.xml
     # The output here is the secret for this libvirt client with ceph save it to a file client.libvirt
virsh secret-define --file secret.xml > client.libvirt
sed -i -e 's/Secret\ //g' -e 's/\ created//g' client.libvirt

#to undo
virsh secret-undefine 12fc063a-9104-41a0-847b-d0e0cbbf3d5b


3.  Key the secret into ceph  
ceph auth get-key client.libvirt > client.libvirt.key

4. Set UUID of secret:
virsh secret-set-value --secret $(cat client.libvirt) --base64 $(cat client.libvirt.key)



####Adding a ceph volume to a VM

qemu-img create -f rbd rbd:libvirt-pool/kane1-libvirt-image 500G

Retrieve the UUID from /etc/ceph/client.libvirt

virsh edit kane1.hostconfig.mgmt.us1.unicorn.md.fireeye.com
add the following below a </disk> line  except changing the correct secret uuid, pool names, and image names.

<disk type='network' device='disk'>
<driver name='qemu'/>
<auth username='libvirt'>
<secret type='ceph' uuid='{UUID from step 2}'/>
</auth>
<source protocol='rbd' name='{ceph-pool}/{VM disk img}'>
<host name='ceph2.hostconfig.mgmt.us1.dev.md.fireeye.com' port='6789'/>
</source>
<target dev='vdb' bus='virtio'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
</disk>

    <disk type='network' device='disk'>
      <driver name='qemu'/>
      <auth username='libvirt'>
        <secret type='ceph' uuid='66b3d1e9-6b07-413e-b33f-7648890acc9a'/>
      </auth>
      <source protocol='rbd' name='libvirt-pool/cephnas1-libvirt-image'>
        <host name='ceph2.hostconfig.mgmt.us1.dev.md.fireeye.com' port='6789'/>
      </source>
      <target dev='vdb' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>

Reboot VM and storage should appear
mke2fs -t ext4 /dev/sdblah


###Adding a monitor to a pre-existing cluster
from the admin node run
ceph-deploy --username cephuser install ceph3
ceph-deploy --username cephuser mon add ceph3
ceph-deploy --username cephuser mon create ceph3
ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph3.asok mon_status
ceph quorum_status --format json-pretty


###Adding OSDs
First Gather the keys
ceph-deploy gatherkeys ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com



#List the drive on the host
ceph-deploy disk list ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com


Blank OSDs with zap
ceph-deploy disk zap ceph3:sda

Prepare OSDs
ceph-deploy osd prepare {node-name}:{data-disk}[:{journal-disk}]
ceph-deploy --username cephuser osd prepare ceph3:sda:sdg

Activate OSDs
ceph-deploy osd activate {node-name}:{data-disk-partition}[:{journal-disk-partition}]
ceph-deploy --username cephuser osd activate ceph3:sda1:sdg1


#############################################################3
#######################################################


cssh -l nlewis kvm17.hostconfig.mgmt.us1.dev.md.fireeye.com kvm19.dev.md.fireeye.com kvm20.hostconfig.mgmt.us1.dev.md.fireeye.com kvm23.hostconfig.mgmt.us1.dev.md.fireeye.com

libvirt-1.2.8-16.el7_1.3.faas.x86_64 libvirt-client-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-config-network-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-config-nwfilter-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-interface-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-lxc-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-network-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-nodedev-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-nwfilter-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-qemu-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-secret-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-storage-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-kvm-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-lxc-1.2.8-16.el7_1.3.faas.x86_64 libvirt-debuginfo-1.2.8-16.el7_1.3.faas.x86_64 libvirt-devel-1.2.8-16.el7_1.3.faas.x86_64 libvirt-docs-1.2.8-16.el7_1.3.faas.x86_64 libvirt-lock-sanlock-1.2.8-16.el7_1.3.faas.x86_64 libvirt-login-shell-1.2.8-16.el7_1.3.faas.x86_64

17 prod root
19 3val
20
23







