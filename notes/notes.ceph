###Removing a monitor

when recreating a cluster with a box that used to be in a different cluster remove these packages

service ceph -a stop mon.{mon-id}
Remove the monitor from the cluster.
ceph mon remove {mon-id}
Remove the monitor entry from ceph.conf.

ceph-deploy purgedata {ceph-node} [{ceph-node}]
ceph-deploy forgetkeys
ceph-deploy purge {ceph-node} [{ceph-node}]

yum -y remove python-rados
yum -y remove librados2
yum -y remove python-rbd
yum -y remove python-cephfs
yum -y remove libcephfs1
yum -y remove radosgw

####Removing OSDs
From admin node
ceph osd out {osd-num}
ceph -w #observe effects
From OSD host
ssh {osd-host}
sudo /etc/init.d/ceph stop osd.{osd-num}
ceph osd crush remove {name}
ceph auth del osd.{osd-num}
ceph osd rm {osd-num}
From Admin node remove all entries in ceph.conf

###Creating CEPH Cluster
http://ceph.com/docs/master/rados/deployment/preflight-checklist/

create a cluster directory in / to hold all the conf files

create ceph user
give user passwordless sudo
Configure your admin machine with password-less SSH access to each node running Ceph daemons

install CEPH deploy
vi /etc/yum.repos.d/ceph.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://ceph.com/rpm-{ceph-release}/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc

 yum install ceph-deploy

create new cluster with new monitors
ceph-deploy new ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com

ceph-deploy install ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com

Add the initial monitor(s) and gather the keys:
ceph-deploy mon create-initial

[root@ceph3 ceph]# ls
ceph.bootstrap-mds.keyring  ceph.bootstrap-rgw.keyring  ceph.conf  ceph.mon.keyring
ceph.bootstrap-osd.keyring  ceph.client.admin.keyring   ceph.log

#Maybe not????
#gather keys
#ceph-deploy gatherkeys ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com

#List the drive on the host
ceph-deploy disk list ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com


Blank OSDs with zap
ceph-deploy disk zap ceph3:sda
ceph-deploy disk zap ceph3:sdb
ceph-deploy disk zap ceph3:sdc
ceph-deploy disk zap ceph3:sdd
ceph-deploy disk zap ceph3:sdg
ceph-deploy disk zap ceph3:sdh
ceph-deploy disk zap ceph3:sdi
ceph-deploy disk zap ceph3:sdj
ceph-deploy disk zap ceph3:sdk
ceph-deploy disk zap ceph3:sdl
ceph-deploy disk zap ceph3:sdm
ceph-deploy disk zap ceph3:sdn


Prepare OSDs
ceph-deploy osd prepare {node-name}:{data-disk}[:{journal-disk}]
ceph-deploy osd prepare ceph3:sda:sdg
ceph-deploy osd prepare ceph3:sdb:sdg
ceph-deploy osd prepare ceph3:sdc:sdg
ceph-deploy osd prepare ceph3:sdd:sdg
ceph-deploy osd prepare ceph3:sdi:sdg

ceph-deploy osd prepare ceph3:sdj:sdh
ceph-deploy osd prepare ceph3:sdk:sdh
ceph-deploy osd prepare ceph3:sdl:sdh
ceph-deploy osd prepare ceph3:sdm:sdh
ceph-deploy osd prepare ceph3:sdn:sdh

Activate OSDs
ceph-deploy osd activate {node-name}:{data-disk-partition}[:{journal-disk-partition}]
ceph-deploy --username cephuser osd activate ceph3:sda1:sdg1
ceph-deploy --username cephuser osd activate ceph3:sdb1:sdg2
ceph-deploy --username cephuser osd activate ceph3:sdc1:sdg3
ceph-deploy --username cephuser osd activate ceph3:sdd1:sdg4
ceph-deploy --username cephuser osd activate ceph3:sdi1:sdg5

ceph-deploy --username cephuser osd activate ceph3:sdj1:sdh1
ceph-deploy --username cephuser osd activate ceph3:sdk1:sdh2
ceph-deploy --username cephuser osd activate ceph3:sdl1:sdh3
ceph-deploy --username cephuser osd activate ceph3:sdm1:sdh4
ceph-deploy --username cephuser osd activate ceph3:sdn1:sdh5

#Use ceph-deploy to copy the configuration file and admin key to your admin node and your Ceph Nodes so that you can use the ceph CLI without having to specify the monitor address and ceph.client.admin.keyring each time you execute a command.
ceph-deploy admin {admin-node} {ceph-node}

set keyring permissions
chmod +r /etc/ceph/ceph.client.admin.keyring

Check your clusterâ€™s health.
ceph health

####Preparing a KVM server to host VMs that mount CEPH volumes

Build KVM with foreman and apply the foremankvm module
reboot

in DevNet from ceph2 the admin-node
switch to the ceph directory 
on ceph2:/my-cluster/
ceph-deploy install kvm18.hostconfig.mgmt.us1.dev.md.fireeye.com
ceph-deploy admin kvm18.hostconfig.mgmt.us1.dev.md.fireeye.com

#on the KVM host
Create a secret for libvirt and ceph to share for auth of ceph shares
in /etc/ceph/
cat > secret.xml <<EOF
<secret ephemeral='no' private='no'>
<usage type='ceph'>
<name>client.libvirt secret</name>
</usage>
</secret>
EOF

Then :  
virsh secret-define --file secret.xml
     # The output here is the secret for this libvirt client with ceph save it to a file client.libvirt
virsh secret-define --file secret.xml > client.libvirt
sed -i -e 's/Secret\ //g' -e 's/\ created//g' client.libvirt

#to undo
virsh secret-undefine 12fc063a-9104-41a0-847b-d0e0cbbf3d5b


3.  Key the secret into ceph  
ceph auth get-key client.libvirt > client.libvirt.key

4. Set UUID of secret:
virsh secret-set-value --secret $(cat client.libvirt) --base64 $(cat client.libvirt.key)



####Adding a ceph volume to a VM

qemu-img create -f rbd rbd:libvirt-pool/kane1-libvirt-image 500G

Retrieve the UUID from /etc/ceph/client.libvirt

virsh edit kane1.hostconfig.mgmt.us1.unicorn.md.fireeye.com
add the following below a </disk> line  except changing the correct secret uuid, pool names, and image names.

<disk type='network' device='disk'>
<driver name='qemu'/>
<auth username='libvirt'>
<secret type='ceph' uuid='{UUID from step 2}'/>
</auth>
<source protocol='rbd' name='{ceph-pool}/{VM disk img}'>
<host name='ceph2.hostconfig.mgmt.us1.dev.md.fireeye.com' port='6789'/>
</source>
<target dev='vdb' bus='virtio'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
</disk>

    <disk type='network' device='disk'>
      <driver name='qemu'/>
      <auth username='libvirt'>
        <secret type='ceph' uuid='66b3d1e9-6b07-413e-b33f-7648890acc9a'/>
      </auth>
      <source protocol='rbd' name='libvirt-pool/cephnas1-libvirt-image'>
        <host name='ceph2.hostconfig.mgmt.us1.dev.md.fireeye.com' port='6789'/>
      </source>
      <target dev='vdb' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>

Reboot VM and storage should appear
mke2fs -t ext4 /dev/sdblah


###Adding a monitor to a pre-existing cluster
from the admin node run
ceph-deploy --username cephuser install ceph3
ceph-deploy --username cephuser mon add ceph3
ceph-deploy --username cephuser mon create ceph3
ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph3.asok mon_status
ceph quorum_status --format json-pretty


###Adding OSDs
First Gather the keys
ceph-deploy gatherkeys ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com



#List the drive on the host
ceph-deploy disk list ceph3.hostconfig.mgmt.us1.dev.md.fireeye.com


Blank OSDs with zap
ceph-deploy disk zap ceph3:sda

Prepare OSDs
ceph-deploy osd prepare {node-name}:{data-disk}[:{journal-disk}]
ceph-deploy --username cephuser osd prepare ceph3:sda:sdg

Activate OSDs
ceph-deploy osd activate {node-name}:{data-disk-partition}[:{journal-disk-partition}]
ceph-deploy --username cephuser osd activate ceph3:sda1:sdg1


#############################################################3
#######################################################


cssh -l nlewis kvm17.hostconfig.mgmt.us1.dev.md.fireeye.com kvm19.dev.md.fireeye.com kvm20.hostconfig.mgmt.us1.dev.md.fireeye.com kvm23.hostconfig.mgmt.us1.dev.md.fireeye.com

libvirt-1.2.8-16.el7_1.3.faas.x86_64 libvirt-client-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-config-network-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-config-nwfilter-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-interface-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-lxc-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-network-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-nodedev-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-nwfilter-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-qemu-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-secret-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-driver-storage-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-kvm-1.2.8-16.el7_1.3.faas.x86_64 libvirt-daemon-lxc-1.2.8-16.el7_1.3.faas.x86_64 libvirt-debuginfo-1.2.8-16.el7_1.3.faas.x86_64 libvirt-devel-1.2.8-16.el7_1.3.faas.x86_64 libvirt-docs-1.2.8-16.el7_1.3.faas.x86_64 libvirt-lock-sanlock-1.2.8-16.el7_1.3.faas.x86_64 libvirt-login-shell-1.2.8-16.el7_1.3.faas.x86_64

17 prod root
19 3val
20
23


#############CRUSH MAP
in ceph.conf
[global]
osd pool default size = 3
osd pool default min size = 2

# the number of OSDs * 100 / {osd pool default size} = 1000
osd pool default pg num = 1000
osd pool default pgp num = 1000

max open files = 9,223,372,036,854,775,807 #maximum 64-bit number

replicated




####Pools
ceph osd pool set {pool-name} {key} {value}
size 3
min_size 2
crush_ruleset

ceph osd pool set libvirt-pool crush_ruleset 2

ceph osd crush tunables optimal

ceph osd setcrushmap -i /tmp/crush.new



TODO SingTel cephmon to kvm communication
assign DHCP IP to br206
add "if I can't resolve myself do nsupdate" to puppet manifest
use kvm01.ceph in configs

sudo -u cephuser ceph-deploy --username cephuser disk list

sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdc
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdd
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sde
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdf
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdg
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdh
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdi
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdj
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdk
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdl
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdm
sudo -u cephuser ceph-deploy --username cephuser disk zap kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdn

sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sde:sdc
sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdf:sdc
sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdg:sdc
sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdh:sdc
sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdi:sdc

sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdj:sdd
sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdk:sdd
sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdl:sdd
sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdm:sdd
sudo -u cephuser ceph-deploy --username cephuser osd prepare kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdn:sdd


sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sde1:sdc1
sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdf1:sdc2
sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdg1:sdc3
sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdh1:sdc4
sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdi1:sdc5

sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdj1:sdd1
sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdk1:sdd2
sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdl1:sdd3
sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdm1:sdd4
sudo -u cephuser ceph-deploy --username cephuser osd activate kvm02.ceph.mgmt.sin1.prod.md.fireeye.com:sdn1:sdd5



for i in {1..500}; do dd if=/dev/urandom bs=1024 count=102400 of=file$i; done
~.
mount -t fuse.ceph conf=/home/kane/ceph/ceph.conf /media/ceph/

ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]
ceph pg dump
ceph pg dump -o {filename} --format=json
ceph pg {poolnum}.{pg-id} query
ceph pg force_create_pg 

possible tuning options
osd op threads = 8
osd max backfills = 1
osd recovery max active = 1
filestore max sync interval = 50
filestore min sync interval = 20
filestore queue max ops = 2000
filestore queue max bytes = 536870912
filestore queue committing max ops = 2000
filestore queue committing max bytes = 536870912

osd recovery delay start

osd recovery op priority

osd op threads default is 2
