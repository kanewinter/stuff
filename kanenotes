streamer -c /dev/video0 -b 16 -o outfile.jpeg

http://www.systutorials.com/4687/mounting-google-drive-on-linux/

touch /tmp/checkpoint
<do installer stuff>
find / -newer /tmp/checkpoint

FireEye VPN CSD ISSUE
openconnect --csd-user=kane --no-xmlpost https://fevpn.fireeye.com

grep package basenode.pp | sed -e 's/":.*$//' -e 's/.*"//' | xargs

check_postgres
yum install check_postgres perl-Time-HiRes

https://github.mandiant.com/HTAP/seasick/blob/develop/vagrant/vagrant.pp

puppet cert --clean your.new.server.mandiant.com
rm -rf /var/lib/puppet/ssl ; rm -rf /etc/puppet/ssl

ntpdate 0.pool.ntp.org

UPDATE your branch
git fetch upstream
git checkout master
git merge upstream/master
git push

THEN
make changes to the fork of the mplex puppet manifest
git push
create merge request

merge pull request
ssh root@puppet.mplex.us2 "cd /etc/puppet;git pull"

push to a new devnet branch
git push devnet master:foremankvm

Set up environment hooks etc on a puppet server
puppet apply --debug --test --modulepath /opt/prodpuppet/modules --noop -e "class {'puppet::git_environments': git_group => 'unicorn',}"
git push unicorn bind



LDAPS setup
http://support.microsoft.com/kb/321051


[6:27:41 PM] mat_oldham: when moving aim sensors over we need to do the following:
[6:27:48 PM] mat_oldham: update /etc/hosts to point collector to the appropriate location
[6:27:59 PM] mat_oldham: remove the collector entry from the known_hosts file
[6:28:39 PM] Kane Lewis: update /etc/hosts on the sensor or collector
[6:28:45 PM] mat_oldham: sensor
[6:28:54 PM] mat_oldham: longer term, we should just manage that with puppet
[6:28:59 PM] Kane Lewis: yup
[6:29:31 PM] Kane Lewis: so we'd have to run file updater and accept the new key
[6:30:21 PM] mat_oldham: if we change teh actual hostname on the sensor, we'll need to gen and push up a new key
[6:32:32 PM] mat_oldham: on collector1 i had to do the following:
[6:32:33 PM] mat_oldham: AuthorizedKeysFile      .ssh/authorized_keys
[6:39:30 PM] mat_oldham: sensor is attempting to connect, sshd is setup appropriately
[6:39:43 PM] mat_oldham: though, it requires a password, it's not finding hte public key

Moving Hops sensors:
1. change openvpn to point to the new IP 205.233.0.23 1201
    /usr/local/etc/openvpn/openvpn.conf 
2. update /etc/hosts to point collector 172.22.32.5
3. replace the collector entry from the known_hosts
4. on collector add sensor to AuthorizedKeysFile      .ssh/authorized_keys

/usr/local/etc/rc.d/openvpn restart

. /etc/mandiant.conf
scp /home/$NSM/.ssh/id_rsa.pub root@collector:/home/nsm_transfer/.ssh/${SENSORNAME}.pub

cat *.pub > authorized_keys

sed -i '' -e 's/.0./.32./' /etc/resolv.conf
cat /etc/resolv.conf

pkg_add -r nsca-client


tcpstat -i eth0 -s 7200 60 > output.file.txt
tcpstat -s 7200 60 -pf "host 172.19.0.14"

##Local puppet manifest apply
puppet apply -l /tmp/manifest.log manifest.pp

###WHEN eth0 is missing on a VM and otherwise
# rm /etc/udev/rules.d/70-persistent-net.rules
# reboot

###DRAC command line
http://thornelabs.net/2014/04/16/dell-idrac-racadm-commands-and-scripts.html




####setting up bond and bridge
https://www.centos.org/forums/viewtopic.php?f=47&t=48043

###Add to /etc/dhcp/dhcpd.conf per added subnet
subnet 172.20.128.0 netmask 255.255.255.0 {
  pool
  {
    range 172.20.128.120 172.20.128.150;
  }

  option subnet-mask 255.255.255.0;
  option routers 172.20.128.1;
}
####comment out setting in /etc/foreman-proxy/settings.d/dhcp.yml
#:dhcp_subnets: [172.20.128.0/255.255.255.0, 172.20.129.0/255.255.255.0, 172.20.131.0/255.255.255.0]
#so foreman can load more subnets

####adding domains to DNS
#add something like this to /etc/zones.conf
zone "129.20.172.in-addr.arpa" {
    type master;
    file "/var/named/dynamic/db.129.20.172.in-addr.arpa";
    update-policy {
            grant rndc-key zonesub ANY;
    };
};
zone "blah.unicorn.mcirt.mandiant.com" {
    type master;
    file "/var/named/dynamic/db.blah.unicorn.mcirt.mandiant.com";
    update-policy {
            grant rndc-key zonesub ANY;
    };
};


##building team0 on KVMs
edit ifcfg-*
DEVICE1=enp8s0f1
DEVICE2=enp8s0f0
echo "DEVICETYPE="BondPort"" >> ifcfg-$DEVICE1
echo "TEAM_MASTER="team0"" >> ifcfg
sed -i s/^BOOTPROTO.*/BOOTPROTO="none"/g ifcfg
sed -i s/^ONBOOT.*/ONBOOT="yes"/g
echo "DEVICE="team0"
DEVICETYPE="Team"
ONBOOT="yes"
BOOTPROTO="dhcp"
TEAM_CONFIG='{"runner": {"name": "roundrobin"}}'" > ifcfg-team0

###DevNet-MPLEX
echo "VLAN=yes
DEVICE=team0.160
BOOTPROTO=dhcp
ONBOOT=yes" > ifcfg-DevNet-MPLEX

or use vconfig
yum install vconfig
vconfig add team0 160
NEVERMIND doesn't survive reboot

sed -i 's/bond/br/g' ifcfg-QAnet-*
sed -i 's/bond/br/g' ifcfg-DevNet-*


####If you have difficulty connecting, test access using the virsh command under the 'foreman' account on the Foreman host first, e.g. virsh -c qemu+ssh://hypervisor.example.com/system list

##Foreman stuck in pending build
wget -q -O /dev/null --no-check-certificate http://foreman.lab.unicorn.mcirt.mandiant.com/unattended/built

#####Adding compute resource test
sudo -u foreman ssh foreman@kvm09.hostconfig.mgmt.us1.dev.md.fireeye.com

###compute profiles won't work until you connect to each kvm via virtmanager

####Foreman Hammer
http://blog.theforeman.org/2013/11/hammer-cli-for-foreman-part-i-setup.html
hammer domain info --name dev.portal.us1.mcirt.mandiant.com
hammer proxy list
##gives us something like:
hammer subnet create --dhcp-id "1" --dns-id "1" --dns-primary 172.20.166.1 --domain-ids "7" --from 172.20.166.50 --gateway 172.20.166.1 --ipam TRUE --mask 255.255.255.0 --name DevNet-PORTAL --network 172.20.166.0 --tftp-id "1" --to 172.20.166.200 --vlanid 166
###make that easy to manipulate
hammer subnet create --dhcp-id "1" --dns-id "1" --tftp-id "1" --ipam TRUE --mask 255.255.255.0 --domain-ids "7" --network 172.20.166.0 --vlanid 166 --dns-primary 172.20.166.1 --gateway 172.20.166.1 --from 172.20.166.50 --to 172.20.166.200 --name DevNet-PORTAL

hammer subnet create --dhcp-id "1" --dns-id "1" --tftp-id "1" --ipam TRUE --mask 255.255.255.0 --domain-ids "11" --network 172.20.182.0 --vlanid 182 --dns-primary 172.20.182.1 --gateway 172.20.182.1 --from 172.20.182.50 --to 172.20.182.200 --name QAnet-PORTAL

###use hammer csv to import and export


###Foreman setup post Geordie
IPA needs global forwarder for DNS

foreman-installer --enable-foreman-plugin-discovery
##for the proxy servers
yum install rubygem-smart_proxy_discovery

follow discovery directions

install foreman-proxy on puppet
remember this:
https://groups.google.com/forum/#!topic/foreman-users/5fhXp7e_u5Q
foreman-proxy might not listen to anything but localhost


fix foreman-proxy ssl certs under settings provisioning
import puppet stuff
make sure proxies are installed for IPA, puppet

check foreman into puppet
foreman.yaml in /etc/puppet on PUPPETMASTER needs to point at ipa certs for foreman communication

fix foreman ssh config to connect first time



####Singapore TO DO before leaving
network wiring is complete and correct
boxes look like what we ordered
all dracs work
all raids configured


###to do extra credit
ipa
OS on each box
puppet
foreman
vlans




Archipel:
First Draft:
install EPEL
yum install httpd subversion erlang-xmerl erlang-xmlrpc erlang-tools
http://www.process-one.net/en/ejabberd/downloads
https://github.com/ArchipelProject/Archipel/wiki/Ejabberd%3A-Configuration
install mod_admin_extra
https://www.ejabberd.im/ejabberd-modules

Second Draft:
install using mandiant iso
run puppet and make sure epel is installed
groupadd ejabberd
yum install git gcc openssl-devel expat-devel
cd /root/
wget http://packages.erlang-solutions.com/site/esl/esl-erlang/FLAVOUR_1_general/esl-erlang_17.1-1~centos~6_amd64.rpm
yum localinstall esl-erlang_17.1-1~centos~6_amd64.rpm
wget http://www.process-one.net/downloads/downloads-action.php?file=/ejabberd/14.07/ejabberd-14.07.tgz
tar -xvf ejabberd-14.07.tgz
yum install libyaml libyaml-devel (found the RPMs and installed manually)
cd ejabberd-14.07
### where "ejabber" is the unpriviledged user that will run the ejabberd service
./configure --enable-user=ejabberd
./configure --enable-user=ejabberd --with-openssl=/usr/lib64/openssl
make && make install
ejabberdctl register admin host.domain password
configure the jabber configs




agent
yum install libvirt-python python-imaging python-devel
easy_install numpy sqlalchemy apscheduler
easy_install archipel-agent
archipel-initinstall
https://github.com/ArchipelProject/Archipel/wiki/Installation%3A-archipel-agent%27s-configuration-file

archipel-tagnode --jid=admin@archipel --password=010806Kl --create
archipel-rolesnode --jid=admin@archipel --password=010806Kl --create
archipel-adminaccounts --jid=admin@archipel --password=010806Kl --create
create a startup script
runarchipel

/opt/ejabberd-14.12/bin/ejabberdctl connected_users
go to archipel and add contact, add hypervisor



GIT
start a new repo



HSVDR22


vm8
The virtual machine requires hardware features that are unsupported or disabled on the target host:
* Carryless multiply (PCLMULQDQ) (see KB 1034926)
* AES instructions (AES-NI) (see KB 1034926)

If possible, use a cluster with Enhanced vMotion Compatibility (EVC) enabled; see KB article 1003212.

CPUID details: incompatibility at level 0x1 register 'ecx'.
Host bits: 0000:0000:1001:1000:0010:0010:0000:0001
Required:  x0xx:xx1x:10x1:1xx0:xxxx:xx1x:xxxx:xx11


# You cannot "clone" a running vm, stop it.  suspend and destroy
# are also valid options for less graceful cloning
virsh shutdown this.vm

# copy the storage.
cp /var/lib/libvirt/images/{this-vm,that-vm}.img

# dump the xml for the original
virsh dumpxml this-vm > /tmp/that-vm.xml

# hardware addresses need to be removed, libvirt will assign
# new addresses automatically
sed -i /uuid/d /tmp/that-vm.xml
sed -i '/mac address/d' /tmp/that-vm.xml

# and actually rename the vm: (this also updates the storage path)
sed -i s/this-vm/that-vm /tmp/that-vm.xml

# finally, create the new vm
virsh define /tmp/that-vm.xml
virsh start this-vm
virsh start that-vm
